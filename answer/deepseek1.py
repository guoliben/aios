import os
os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# model_name = "deepseek-ai/deepseek-coder-1.3b-instruct"
model_name = "deepseek-coder-1.3b-instruct"



tokenizer = AutoTokenizer.from_pretrained("deepseek-ai/deepseek-coder-1.3b-instruct", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    "deepseek-ai/deepseek-coder-1.3b-instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True
)
#
# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
# model = AutoModelForCausalLM.from_pretrained(
#     model_name,
#     torch_dtype=torch.float16,
#     device_map="auto",  # ä¼šæ ¹æ® MPS/CPU è‡ªåŠ¨è°ƒåº¦
#     trust_remote_code=True
# )

while True:
    user_input = input("ğŸ§‘ ä½ ï¼š")
    if user_input.lower() in ["exit", "quit", "bye"]:
        print("ğŸ‘‹ å†è§ï¼")
        break
    prompt = "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=128, do_sample=True, temperature=0.7)
    print(tokenizer.decode(outputs[0], skip_special_tokens=True))




'''
import os
os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_name = "deepseek-ai/deepseek-coder-1.3b-instruct"

print("ğŸš€ æ¨¡å‹åŠ è½½ä¸­ï¼Œè¯·ç¨å€™...")

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True
)

print("âœ… æ¨¡å‹å·²åŠ è½½ï¼Œå¼€å§‹å¯¹è¯å§ï¼è¾“å…¥ 'exit' é€€å‡ºã€‚\n")

history = ""

while True:
    user_input = input("ğŸ§‘ ä½ ï¼š")
    if user_input.lower() in ["exit", "quit", "bye"]:
        print("ğŸ‘‹ å†è§ï¼")
        break

    # æ„é€  Promptï¼ˆå¯é€‰ï¼šä¿ç•™å†å²å¯¹è¯ä»¥æ”¯æŒä¸Šä¸‹æ–‡ï¼‰
    prompt = history + f"\nç”¨æˆ·ï¼š{user_input}\nåŠ©æ‰‹ï¼š"

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(
        **inputs,
        max_new_tokens=256,
        do_sample=True,
        temperature=0.7,
        pad_token_id=tokenizer.eos_token_id
    )
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # æˆªå–å›ç­”éƒ¨åˆ†ï¼ˆå»é™¤ Promptï¼‰
    answer = response[len(prompt):].strip()

    print(f"ğŸ¤– åŠ©æ‰‹ï¼š{answer}\n")

    # æ›´æ–°å†å²è®°å½•
    history += f"\nç”¨æˆ·ï¼š{user_input}\nåŠ©æ‰‹ï¼š{answer}"
'''

